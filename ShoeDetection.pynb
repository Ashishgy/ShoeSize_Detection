{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPwpxTa3q60Z6abmrj7b1A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashishgy/ShoeSize_Detection/blob/main/ShoeDetection.pynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kqjcv_j30vTY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def tiled_matrix_multiply(A, B, tile_size):\n",
        "    \"\"\"\n",
        "    Perform tiled matrix multiplication on two matrices A and B.\n",
        "    Args:\n",
        "        A (numpy.ndarray): Input matrix of shape (M, N)\n",
        "        B (numpy.ndarray): Input matrix of shape (N, P)\n",
        "        tile_size (int): Size of the tiles for blocking\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Resultant matrix after tiled multiplication\n",
        "    \"\"\"\n",
        "    M, N = A.shape\n",
        "    _, P = B.shape\n",
        "    C = np.zeros((M, P))\n",
        "\n",
        "    for i in range(0, M, tile_size):\n",
        "        for j in range(0, P, tile_size):\n",
        "            for k in range(0, N, tile_size):\n",
        "                A_tile = A[i:i+tile_size, k:k+tile_size]\n",
        "                B_tile = B[k:k+tile_size, j:j+tile_size]\n",
        "                if A_tile.shape[1] == B_tile.shape[0]:\n",
        "                    C[i:i+tile_size, j:j+tile_size] += np.dot(A_tile, B_tile)\n",
        "    return C\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Kn2RowConv2D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, tile_size=4):\n",
        "        super(Kn2RowConv2D, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.tile_size = tile_size\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
        "        )\n",
        "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, _, height, width = x.shape\n",
        "        output_height = height - self.kernel_size + 1\n",
        "        output_width = width - self.kernel_size + 1\n",
        "        output = torch.zeros((batch_size, self.out_channels, output_height, output_width)).to(x.device)\n",
        "\n",
        "        for i in range(output_height):\n",
        "            for j in range(output_width):\n",
        "                patch = x[:, :, i:i+self.kernel_size, j:j+self.kernel_size]\n",
        "                patch_flat = patch.reshape(batch_size, -1).cpu().numpy()\n",
        "                kernel_flat = self.weight.view(self.out_channels, -1).cpu().numpy()\n",
        "                result = np.zeros((batch_size, self.out_channels))\n",
        "                for b in range(batch_size):\n",
        "                    result[b, :] = tiled_matrix_multiply(patch_flat[b:b+1], kernel_flat.T, self.tile_size)\n",
        "                output[:, :, i, j] = torch.from_numpy(result).to(x.device) + self.bias\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "34NNLFc13wgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KLab-AI3/ai3.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjOQwNlG3zQs",
        "outputId": "5705a3e0-a52a-45a6-afc8-68d9d2bbd0eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai3'...\n",
            "remote: Enumerating objects: 2895, done.\u001b[K\n",
            "remote: Counting objects: 100% (549/549), done.\u001b[K\n",
            "remote: Compressing objects: 100% (227/227), done.\u001b[K\n",
            "Receiving objects: 100% (2895/2895), 2.25 MiB | 1.17 MiB/s, done.\n",
            "remote: Total 2895 (delta 314), reused 508 (delta 283), pack-reused 2346 (from 1)\u001b[K\n",
            "Resolving deltas: 100% (1852/1852), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ai3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPCVNcD29GCd",
        "outputId": "d57f3d80-e5c3-4e63-ec53-1031bc0a9d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ai3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF1780Sh9NmC",
        "outputId": "52147f6d-d788-435d-effb-adc0bfe8b37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/ai3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from aithree==0.1.1.dev43+gba1f94a) (24.2)\n",
            "Building wheels for collected packages: aithree\n",
            "  Building wheel for aithree (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aithree: filename=aithree-0.1.1.dev43+gba1f94a-cp310-cp310-linux_x86_64.whl size=210162 sha256=557accaa29c45ab90fb2445f52435c926f93d71128608e2855c2a5cdcde63210\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z63_pdip/wheels/ce/ca/00/fcb63f9c96e189c6705081053bd64df2530a49bd2a1ab75052\n",
            "Successfully built aithree\n",
            "Installing collected packages: aithree\n",
            "Successfully installed aithree-0.1.1.dev43+gba1f94a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ai3\n",
        "print(\"SYCL available:\", ai3.using_sycl())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "C6lYQfBH9QEm",
        "outputId": "367d3882-27af-4091-e5b4-ff079b556f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'ai3' has no attribute 'using_sycl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f8cefd06c25b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mai3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SYCL available:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mai3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musing_sycl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'ai3' has no attribute 'using_sycl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import ai3\n",
        "    print(\"AI3 module imported successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing AI3: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLUvS96O5p5r",
        "outputId": "99cf0896-7985-4abe-ad62-bf27323fded5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI3 module imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ai3\n",
        "print(dir(ai3))  # Lists top-level modules in ai3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HXkzMFU6BEa",
        "outputId": "5c62dbac-47c9-4666-a511-428f8c8be959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AlgorithmicSelector', 'DEFAULT_ALGOS', 'FROM_BACKEND', 'Mapping', 'Model', 'Optional', 'SUPPORTED_ALGORITHMS', 'SUPPORTED_FROM_BACKENDS', 'Sequence', 'Tensor', 'Type', 'Union', '__annotations__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_core', '_version', 'convert', 'errors', 'layers', 'swap_conv2d', 'swap_operation', 'tensor', 'using_cublas', 'using_cudnn', 'using_mps_and_metal', 'using_sycl', 'utils']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show aithree  # Check the version of AI3\n",
        "!pip show ai3      # Sometimes, AI3 might have different names installed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDIsa9Oh6TO4",
        "outputId": "8aae3128-ee0b-4f6f-b98c-d16b858ac5a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: aithree\n",
            "Version: 0.1.1.dev41+g55d79a3\n",
            "Summary: Enables Algorithmic Selection and Customization in Deep Neural Networks\n",
            "Home-page: \n",
            "Author: Timothy Cronin\n",
            "Author-email: \n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: packaging\n",
            "Required-by: \n",
            "\u001b[33mWARNING: Package(s) not found: ai3\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect all available attributes in the ai3 module\n",
        "print(dir(ai3))\n",
        "\n",
        "# Replace 'submodule_name' with any submodule or class name you suspect may contain CustomLayerBase\n",
        "try:\n",
        "    print(dir(ai3.Model))  # Example, replace 'Model' with another element from dir(ai3)\n",
        "except AttributeError as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNLZ0Az86lUj",
        "outputId": "df9980a2-6608-44df-9c73-7f8b0f8cabbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AlgorithmicSelector', 'DEFAULT_ALGOS', 'FROM_BACKEND', 'Mapping', 'Model', 'Optional', 'SUPPORTED_ALGORITHMS', 'SUPPORTED_FROM_BACKENDS', 'Sequence', 'Tensor', 'Type', 'Union', '__annotations__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_core', '_version', 'convert', 'errors', 'layers', 'swap_conv2d', 'swap_operation', 'tensor', 'using_cublas', 'using_cudnn', 'using_mps_and_metal', 'using_sycl', 'utils']\n",
            "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'predict']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_selector(orig: torch.nn.Conv2d, input_shape) -> str:\n",
        "    # Replace conditions with logic for selecting Kn2RowConv2D\n",
        "    if orig.weight.shape[0] < 64:  # Example condition based on out_channels\n",
        "        return 'kn2row'  # Your custom layer identifier\n",
        "    return 'default'\n"
      ],
      "metadata": {
        "id": "KDp8Xy-O9nt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Device Name:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"No GPU available.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTJvbd7kBTj8",
        "outputId": "f99d8992-b0f3-4728-c1a2-90e586186ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: True\n",
            "Device Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 3, 224, 224).cuda()  # Allocate a tensor on the GPU\n",
        "print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ktm1CUvB8zY",
        "outputId": "98dc245c-1ff6-4b23-e327-ecd5a0bf5a78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-1.1333, -0.6225,  1.2589,  ...,  0.3850, -0.1571,  1.3608],\n",
            "          [-0.8926,  0.0979, -1.1809,  ..., -0.4242, -0.9301,  1.2949],\n",
            "          [ 0.7827,  0.0434, -0.4755,  ...,  0.4912,  1.2967,  1.9732],\n",
            "          ...,\n",
            "          [ 0.6648,  3.5491, -1.2949,  ..., -0.1719,  0.3528,  0.6527],\n",
            "          [ 1.5201,  0.4444, -0.2348,  ..., -0.9853,  0.0140, -1.6299],\n",
            "          [-0.3857,  0.0757, -0.8600,  ..., -0.1934, -0.4202,  0.2986]],\n",
            "\n",
            "         [[ 0.5053,  1.0663, -1.5253,  ..., -0.7076,  0.1849, -1.0714],\n",
            "          [ 0.6356, -2.0651, -0.4954,  ..., -0.3259,  0.9171, -0.0283],\n",
            "          [ 0.5759,  1.2492, -0.1867,  ..., -0.1382,  0.1565, -1.2226],\n",
            "          ...,\n",
            "          [ 0.4618, -0.4020,  1.4548,  ...,  0.3646, -0.4321,  0.1837],\n",
            "          [ 0.4040, -1.7106, -1.0397,  ..., -0.2079,  0.0117, -0.0307],\n",
            "          [ 0.0465, -2.1221, -1.9187,  ..., -1.0234,  2.3839,  1.1323]],\n",
            "\n",
            "         [[-0.1619,  1.0789,  1.6024,  ..., -0.9592, -0.8061, -0.5897],\n",
            "          [-1.8202, -0.5129, -1.4801,  ...,  0.7960,  1.4355, -0.3144],\n",
            "          [ 0.4430,  0.2985,  0.4235,  ...,  0.8828, -0.3278, -0.2500],\n",
            "          ...,\n",
            "          [-0.9870,  0.3235, -0.5085,  ..., -1.5488,  0.0662, -2.3885],\n",
            "          [-0.3101,  0.7977,  0.0674,  ...,  0.3536,  0.2818,  0.2667],\n",
            "          [ 1.0756, -1.2144,  0.1503,  ...,  1.4737, -0.3879,  0.0610]]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Kn2RowConv2D(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, tile_size=4):\n",
        "        super(Kn2RowConv2D, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.tile_size = tile_size\n",
        "        self.weight = torch.nn.Parameter(\n",
        "            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
        "        )\n",
        "        self.bias = torch.nn.Parameter(torch.zeros(out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, _, height, width = x.shape\n",
        "        output_height = height - self.kernel_size + 1\n",
        "        output_width = width - self.kernel_size + 1\n",
        "        output = torch.zeros((batch_size, self.out_channels, output_height, output_width)).to(x.device)\n",
        "\n",
        "        for i in range(output_height):\n",
        "            for j in range(output_width):\n",
        "                patch = x[:, :, i:i+self.kernel_size, j:j+self.kernel_size]\n",
        "                patch_flat = patch.reshape(batch_size, -1).cpu().numpy()\n",
        "                kernel_flat = self.weight.view(self.out_channels, -1).cpu().numpy()\n",
        "                result = np.zeros((batch_size, self.out_channels))\n",
        "                for b in range(batch_size):\n",
        "                    result[b, :] = self.tiled_matrix_multiply(patch_flat[b:b+1], kernel_flat.T)\n",
        "                output[:, :, i, j] = torch.from_numpy(result).to(x.device) + self.bias\n",
        "        return output\n",
        "\n",
        "    def tiled_matrix_multiply(self, A, B):\n",
        "        M, N = A.shape\n",
        "        _, P = B.shape\n",
        "        C = np.zeros((M, P))\n",
        "\n",
        "        for i in range(0, M, self.tile_size):\n",
        "            for j in range(0, P, self.tile_size):\n",
        "                for k in range(0, N, self.tile_size):\n",
        "                    A_tile = A[i:i+self.tile_size, k:k+self.tile_size]\n",
        "                    B_tile = B[k:k+self.tile_size, j:j+self.tile_size]\n",
        "                    C[i:i+self.tile_size, j:j+self.tile_size] += np.dot(A_tile, B_tile)\n",
        "        return C\n"
      ],
      "metadata": {
        "id": "9DSpjQrRC05T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_selector(orig: torch.nn.Conv2d, input_shape):\n",
        "    # Replace conv2d with your custom Kn2RowConv2D\n",
        "    if orig.out_channels < 64:  # Example condition\n",
        "        return 'custom'\n",
        "    return 'default'\n"
      ],
      "metadata": {
        "id": "hcCk9kXGC25O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import ai3\n",
        "\n",
        "# Load the VGG16 model and ensure it is on GPU\n",
        "vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT).eval().cuda()\n",
        "\n",
        "# Generate input data and move it to the GPU\n",
        "input_data = torch.randn(1, 3, 224, 224).cuda()\n",
        "\n",
        "# Confirm that the model is on the GPU\n",
        "print(\"Model device:\", next(vgg16.parameters()).device)\n",
        "\n",
        "# Confirm that the input is on the GPU\n",
        "print(\"Input device:\", input_data.device)\n",
        "\n",
        "# Test the model to ensure it works\n",
        "output = vgg16(input_data)\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjUP1CJvC7uI",
        "outputId": "ea9b740d-7295-4af0-de39-dfa1f963aa60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model device: cuda:0\n",
            "Input device: cuda:0\n",
            "Output shape: torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade aithree\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BtjZShWEdeI",
        "outputId": "6a297478-248b-4618-92e4-1398137710dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aithree in /usr/local/lib/python3.10/dist-packages (0.1.1.dev41+g55d79a3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from aithree) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import ai3\n",
        "\n",
        "# Load the VGG16 model and move it to GPU\n",
        "vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT).eval().cuda()\n",
        "\n",
        "# Define layer selector for AI3 conversion\n",
        "def auto_selector(orig: torch.nn.Conv2d, input_shape):\n",
        "    if orig.out_channels < 64:  # Example condition\n",
        "        return 'custom'\n",
        "    return 'default'\n",
        "\n",
        "# Convert the model using AI3\n",
        "model = ai3.convert(\n",
        "    vgg16,\n",
        "    {'conv2d': auto_selector},  # Replacement logic\n",
        "    sample_input_shape=(1, 3, 224, 224)\n",
        ")\n",
        "\n",
        "# Move the entire converted model to GPU\n",
        "model = model.cuda()\n",
        "\n",
        "# Create input tensor and move it to GPU\n",
        "input_data = torch.randn(1, 3, 224, 224).cuda()\n",
        "\n",
        "# Ensure both input and model are on the same device\n",
        "print(\"Input device:\", input_data.device)\n",
        "print(\"Model device:\", next(model.parameters()).device)\n",
        "\n",
        "# Run the converted model\n",
        "output = model(input_data)\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "Hfqp34vFFZDO",
        "outputId": "27b7005d-9a09-4305-bbb6-cf1ff1e18de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/passes/shape_prop.py\", line 154, in run_node\n",
            "    result = super().run_node(n)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\", line 203, in run_node\n",
            "    return getattr(self, n.op)(n.target, args, kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\", line 320, in call_module\n",
            "    return submod(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
            "    return F.conv2d(\n",
            "RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ShapeProp error for: node=%features_0 : [num_users=1] = call_module[target=features.0](args = (%x,), kwargs = {}) with meta={'nn_module_stack': OrderedDict([('features', ('features', <class 'torch.nn.modules.container.Sequential'>)), ('features.0', ('features.0', <class 'torch.nn.modules.conv.Conv2d'>))])}\n\nWhile executing %features_0 : [num_users=1] = call_module[target=features.0](args = (%x,), kwargs = {})\nOriginal traceback:\nNone",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/passes/shape_prop.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mcall_module\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msubmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-30629da65df4>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Convert the model using AI3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m model = ai3.convert(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mvgg16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m{\u001b[0m\u001b[0;34m'conv2d'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mauto_selector\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Replacement logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ai3/__init__.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(module, algos, sample_input_shape, dtype)\u001b[0m\n\u001b[1;32m    213\u001b[0m     utils.check_callable_params_with_shape(\n\u001b[1;32m    214\u001b[0m         algos, sample_input_shape)\n\u001b[0;32m--> 215\u001b[0;31m     return Model(swapper.convert_layers(\n\u001b[0m\u001b[1;32m    216\u001b[0m         module, dtype, algos, sample_input_shape))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ai3/swap_torch.py\u001b[0m in \u001b[0;36mconvert_layers\u001b[0;34m(complete_module, dtype, algos, sample_input_shape)\u001b[0m\n\u001b[1;32m    277\u001b[0m                    \u001b[0malgos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAlgorithmicSelector\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                    sample_input_shape: Optional[Sequence[int]] = None) -> List[layers.Layer]:\n\u001b[0;32m--> 279\u001b[0;31m     graph, with_shapes = trace_module(\n\u001b[0m\u001b[1;32m    280\u001b[0m         complete_module, sample_input_shape)\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ai3/swap_torch.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(module, sample_input_shape)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_input_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         passes.shape_prop.ShapeProp(gm).propagate(\n\u001b[0m\u001b[1;32m    270\u001b[0m             torch.randn(sample_input_shape))\n\u001b[1;32m    271\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/passes/shape_prop.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mfake_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfake_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/interpreter.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_traceback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/passes/shape_prop.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0;34mf\"ShapeProp error for: node={n.format_node()} with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;34mf\"meta={n.meta}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: ShapeProp error for: node=%features_0 : [num_users=1] = call_module[target=features.0](args = (%x,), kwargs = {}) with meta={'nn_module_stack': OrderedDict([('features', ('features', <class 'torch.nn.modules.container.Sequential'>)), ('features.0', ('features.0', <class 'torch.nn.modules.conv.Conv2d'>))])}\n\nWhile executing %features_0 : [num_users=1] = call_module[target=features.0](args = (%x,), kwargs = {})\nOriginal traceback:\nNone"
          ]
        }
      ]
    }
  ]
}